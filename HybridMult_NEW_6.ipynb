{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBlVPku/1N4kJLuRQd8jwA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steviey/DarwinexLabs/blob/master/HybridMult_NEW_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l1z8ifZiF95f",
        "outputId": "ae0899ce-def7-4913-a7b2-1494f8c221f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Psl-Info: Using Google CoLab\n",
            "Psl-Info: Google Drive is already mounted.\n",
            "Psl-Info: tensorflow is already installed.\n",
            "Psl-Info: numpy is already installed.\n",
            "Psl-Info: cmaes is already installed.\n",
            "Psl-Info: keras-tcn is not installed. Installing...\n",
            "Psl-Info: keras-tuner is not installed. Installing...\n",
            "Psl-Info: pandas is already installed.\n",
            "Psl-Info: pyreadr is already installed.\n",
            "Psl-Info: optuna is already installed.\n",
            "nax_trials:\n",
            "10\n",
            "<class 'numpy.int32'>\n",
            "Optuna version: 4.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2024-11-01 16:25:11,138] Trial 0 failed with parameters: {'layer_type': 'SimpleRNN', 'filters': 104, 'kernel_size': 3, 'layer_size': 37, 'num_heads': 5, 'ff_dim': 49, 'dropout_rate': 0.3346115708152877} because of the following error: ValueError(\"could not convert string to float: '2001-10-05 21:00:00'\").\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-3-77a39759d2e0>\", line 286, in objective\n",
            "    history = model.fit(X_train, Y_train, epochs=setupList['epochs'], batch_size=32,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optree/ops.py\", line 747, in tree_map\n",
            "    return treespec.unflatten(map(func, *flat_args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\", line 6643, in astype\n",
            "    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\", line 430, in astype\n",
            "    return self.apply(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\", line 363, in apply\n",
            "    applied = getattr(b, f)(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py\", line 758, in astype\n",
            "    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\", line 237, in astype_array_safe\n",
            "    new_values = astype_array(values, dtype, copy=copy)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\", line 182, in astype_array\n",
            "    values = _astype_nansafe(values, dtype, copy=copy)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\", line 133, in _astype_nansafe\n",
            "    return arr.astype(dtype, copy=True)\n",
            "ValueError: could not convert string to float: '2001-10-05 21:00:00'\n",
            "[W 2024-11-01 16:25:11,140] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '2001-10-05 21:00:00'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-77a39759d2e0>\u001b[0m in \u001b[0;36m<cell line: 382>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_hp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetupList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetupList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation_split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;31m#history = best_model.fit(X_train, Y_train, epochs=setupList['epochs'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-77a39759d2e0>\u001b[0m in \u001b[0;36moptimize_hyperparameters\u001b[0;34m(X_train, Y_train, input_shape)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m#study.optimize(objective, n_trials=3*3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msetupList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_trials'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mCOLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-77a39759d2e0>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m#                     validation_split=setupList['validation_split'], callbacks=[early_stopping], verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             history = model.fit(X_train, Y_train, epochs=setupList['epochs'], batch_size=32,\n\u001b[0m\u001b[1;32m    287\u001b[0m                                 validation_data=validation_dataP, callbacks=[early_stopping], verbose=0)\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optree/ops.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_is_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_astype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2001-10-05 21:00:00'"
          ]
        }
      ],
      "source": [
        "#%matplotlib inline\n",
        "import warnings, os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    COLAB = True\n",
        "    print(\"Psl-Info: Using Google CoLab\")\n",
        "except:\n",
        "    print(\"Psl-Info: Not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "if COLAB:\n",
        "\n",
        "    if not os.path.ismount('/content/drive'):\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "    else:\n",
        "        print(\"Psl-Info: Google Drive is already mounted.\")\n",
        "\n",
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_if_needed(package_name):\n",
        "    try:\n",
        "        importlib.import_module(package_name)\n",
        "        print(f\"Psl-Info: {package_name} is already installed.\")\n",
        "    except ImportError:\n",
        "        print(f\"Psl-Info: {package_name} is not installed. Installing...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "\n",
        "for package in ['tensorflow', 'numpy', 'cmaes','keras-tcn', 'keras-tuner', 'pandas', 'pyreadr', 'optuna']:\n",
        "    install_if_needed(package)\n",
        "\n",
        "class PathManager:\n",
        "    def __init__(self, base_path):\n",
        "        self.base_path = base_path\n",
        "\n",
        "    def get_script_path(self, file_name):\n",
        "        return f\"{self.base_path}/{file_name}\"\n",
        "\n",
        "if not COLAB:\n",
        "    path_manager = PathManager('/home/rstud/R/adthocStrat')\n",
        "else:\n",
        "    path_manager = PathManager('/content/drive/My Drive/ColabData')\n",
        "\n",
        "import pyreadr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # Set to ERROR to only show errors\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, SimpleRNN, Conv1D, Layer, Input\n",
        "from tensorflow.keras import backend as K\n",
        "from keras_tuner import HyperModel, RandomSearch\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention\n",
        "\n",
        "train_data = pyreadr.read_r(path_manager.get_script_path('python_train_data.Rds'))[None]\n",
        "test_data = pyreadr.read_r(path_manager.get_script_path('python_test_data.Rds'))[None]\n",
        "setup_data = pyreadr.read_r(path_manager.get_script_path('python_setup_data.Rds'))[None]\n",
        "all_data = pyreadr.read_r(path_manager.get_script_path('python_all_data.Rds'))[None]\n",
        "future_data = pyreadr.read_r(path_manager.get_script_path('python_future_data.Rds'))[None]\n",
        "validation_dataP = pyreadr.read_r(path_manager.get_script_path('python_validation_data.Rds'))[None]\n",
        "# from IPython.display import Image\n",
        "\n",
        "# Image(filename='neddwplot (5).png')  # Replace with the actual path/filename\n",
        "\n",
        "# ![My Image](neddwplot (5).png)\n",
        "\n",
        "setupList = {\n",
        "    'exogNames': setup_data['exogNames'].tolist(),\n",
        "    'trainSetup': setup_data['trainSetup'][0],\n",
        "    'time_steps': setup_data['time_steps'][0],\n",
        "    'qRank': setup_data['qRank'][0],\n",
        "    'part': setup_data['part'][0],\n",
        "    'epochs': setup_data['epochs'][0],\n",
        "    'nn_tuner': setup_data['nn_tuner'][0],\n",
        "    'max_trials': setup_data['max_trials'][0],\n",
        "    'patience': setup_data['patience'][0],\n",
        "    'validation_split': setup_data['validation_split'][0],\n",
        "    'executions_per_trial': setup_data['executions_per_trial'][0],\n",
        "    'algoSrc': setup_data['algoSrc'][0],\n",
        "    'layer_size': setup_data['layer_size'][0],\n",
        "    'filters': setup_data['filters'][0],\n",
        "    'ff_dim': setup_data['ff_dim'][0],\n",
        "    'lot_type': setup_data['lot_type'][0],\n",
        "    'editDate': setup_data['editDate'][0],\n",
        "    'sampler': setup_data['sampler'][0],\n",
        "    'dropout_rate': setup_data['dropout_rate'][0],\n",
        "    'kernel_size': setup_data['kernel_size'][0]\n",
        "\n",
        "}\n",
        "\n",
        "# print(train_data.columns)\n",
        "# print(setupList['exogNames'])\n",
        "# sys.exit()\n",
        "# os._exit()\n",
        "\n",
        "#def prepare_input(data, time_steps, exog_names):\n",
        "\n",
        "    #data['ds']  = pd.to_datetime(data['ds'])\n",
        "    #data['ds'] = pd.to_datetime(data['ds']).apply(lambda date: date.toordinal())\n",
        "\n",
        "\n",
        "    # X = []\n",
        "    # Y = []\n",
        "    # for i in range(len(data) - time_steps):\n",
        "    #     X.append(data.iloc[i:(i + time_steps)][exog_names].values)\n",
        "    #     Y.append(data['y'].iloc[i + time_steps])\n",
        "    # return np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n",
        "\n",
        "\n",
        "def prepare_input(data, time_steps, exog_names):\n",
        "    # ... (previous code) ...\n",
        "    data['ds']  = pd.to_datetime(data['ds'])\n",
        "\n",
        "    # Filter only numerical columns from exog_names\n",
        "    numerical_exog_names = data[exog_names].select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for i in range(len(data) - time_steps):\n",
        "        X.append(data.iloc[i:(i + time_steps)][numerical_exog_names].values)\n",
        "        Y.append(data['y'].iloc[i + time_steps])\n",
        "    return np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], input_shape[-1]), initializer=\"glorot_uniform\", trainable=True)\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[-1],), initializer=\"zeros\", trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Dense(ff_dim, activation=\"relu\")\n",
        "        self.ffn_output = Dense(embed_dim)\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.ffn_output(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class Time2Vec(tf.keras.layers.Layer):\n",
        "    def __init__(self, k, **kwargs):\n",
        "        super(Time2Vec, self).__init__(**kwargs)\n",
        "        self.k = k\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(name='w', shape=(input_shape[1],),\n",
        "                                 initializer='uniform', trainable=True)\n",
        "        self.b = self.add_weight(name='b', shape=(input_shape[1],),\n",
        "                                 initializer='uniform', trainable=True)\n",
        "        super(Time2Vec, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        time_linear = self.w * inputs + self.b\n",
        "        time_periodic = tf.math.sin(tf.multiply(inputs, self.k))\n",
        "        return tf.concat([time_linear, time_periodic], axis=-1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({'k': self.k})\n",
        "        return config\n",
        "\n",
        "class HybridHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def positional_encoding(position, d_model):\n",
        "        angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "        # apply sin to even indices in the array; 2i\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "\n",
        "    def build(self, hp):\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "\n",
        "        layer_type = hp.Choice('layer_type', ['Conv1D', 'LSTM', 'GRU', 'SimpleRNN','Transformer','Time2Vec','TransformerPos'])\n",
        "\n",
        "        if layer_type == 'Conv1D':\n",
        "            x = Conv1D(filters=hp.Int('filters', 32, 128),\n",
        "                       kernel_size=hp.Int('kernel_size', 2, setupList['kernel_size']),\n",
        "                       activation='relu')(inputs)\n",
        "\n",
        "        elif layer_type == 'LSTM':\n",
        "            x = LSTM(hp.Int('layer_size',32,setupList['layer_size']), return_sequences=True)(inputs)\n",
        "\n",
        "        elif layer_type == 'GRU':\n",
        "            x = GRU(hp.Int('layer_size', 32,setupList['layer_size']), return_sequences=True)(inputs)\n",
        "\n",
        "        elif layer_type == 'SimpleRNN':\n",
        "            x = SimpleRNN(hp.Int('layer_size', 32,setupList['layer_size']), return_sequences=True)(inputs)\n",
        "\n",
        "        elif layer_type == 'Transformer':\n",
        "            embed_dim = self.input_shape[-1]\n",
        "            num_heads = hp.Int('num_heads', 2, 8)\n",
        "            ff_dim = hp.Int('ff_dim', 32, setupList['ff_dim'])\n",
        "            x = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(inputs)\n",
        "\n",
        "        elif layer_type == 'Time2Vec':\n",
        "            #time_embedding = Time2Vec(input_dim=self.input_shape[1])(inputs)\n",
        "            #x = tf.keras.layers.Concatenate()([inputs, time_embedding])\n",
        "            k = hp.Int('time2vec_k', min_value=1, max_value=10, step=1)  # Hyperparameter for k\n",
        "            x = Time2Vec(k)(inputs)  # Use the custom Time2Vec layer\n",
        "\n",
        "        elif layer_type == 'TransformerPos':\n",
        "            embed_dim = self.input_shape[-1]  # Get the embedding dimension\n",
        "            num_heads = hp.Int('num_heads', 2, 8)\n",
        "            ff_dim = hp.Int('ff_dim', 32, 256)\n",
        "\n",
        "            # Calculate and apply positional encodings\n",
        "            pos_encoding = positional_encoding(self.input_shape[0], embed_dim) #Assuming input_shape=(time_steps, features)\n",
        "            # Add positional encodings to the input\n",
        "            encoded_input = inputs + pos_encoding\n",
        "\n",
        "            x = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(encoded_input)\n",
        "\n",
        "        x = Attention()(x)\n",
        "        #x = Dense(hp.Int('layer_size', 32, 256), activation='relu')(x)\n",
        "        x = Dense(hp.Int('layer_size', 32, setupList['layer_size']), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(hp.Float('l2_reg', 1e-5, 1e-2, sampling='LOG')))(x)\n",
        "        x = Dropout(rate=hp.Float('dropout_rate', 0.1, setupList['dropout_rate']))(x)\n",
        "        outputs = Dense(1)(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "        return model\n",
        "\n",
        "def optimize_hyperparameters(X_train, Y_train, input_shape):\n",
        "    hypermodel = HybridHyperModel(input_shape)  # Instantiate hypermodel\n",
        "\n",
        "    if setupList['nn_tuner'] == 'optuna':\n",
        "        def objective(trial):\n",
        "            hp = {  # Manually create hp object from Optuna trial\n",
        "                'layer_type': trial.suggest_categorical('layer_type', ['Conv1D', 'LSTM', 'GRU', 'SimpleRNN', 'Transformer']),\n",
        "                'filters': trial.suggest_int('filters', 32, setupList['filters']),\n",
        "                'kernel_size': trial.suggest_int('kernel_size', 2, setupList['kernel_size']),\n",
        "                'layer_size': trial.suggest_int('layer_size', 32, setupList['layer_size']),\n",
        "                'num_heads': trial.suggest_int('num_heads', 2, 8),\n",
        "                'ff_dim': trial.suggest_int('ff_dim', 32, setupList['ff_dim']),\n",
        "                'dropout_rate': trial.suggest_float('dropout_rate', 0.1, setupList['dropout_rate']),\n",
        "            }\n",
        "\n",
        "            # This part may need adjustment if HybridHyperModel expects an hp object from keras tuner\n",
        "            # Convert Optuna's trial object to a Keras Tuner HyperParameters object\n",
        "            # This is a workaround and may require you to adjust the structure of the hp dictionary\n",
        "            from keras_tuner.engine import hyperparameters as hp_module\n",
        "            hp_keras = hp_module.HyperParameters()\n",
        "            for k, v in hp.items():\n",
        "                hp_keras.Fixed(k, v)\n",
        "\n",
        "            model = hypermodel.build(hp_keras)  # Pass the hp object to hypermodel.build\n",
        "\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=setupList['patience'])\n",
        "\n",
        "            # history = model.fit(X_train, Y_train, epochs=setupList['epochs'], batch_size=32,\n",
        "            #                     validation_split=setupList['validation_split'], callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "            history = model.fit(X_train, Y_train, epochs=setupList['epochs'], batch_size=32,\n",
        "                                validation_data=validation_dataP, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "            return history.history['val_loss'][-1]\n",
        "\n",
        "        ############################################################\n",
        "        print('nax_trials:')\n",
        "        print(setupList['max_trials'])\n",
        "\n",
        "        print(type(setupList['max_trials']))\n",
        "\n",
        "        print(f\"Optuna version: {optuna.__version__}\")\n",
        "\n",
        "        #optuna.logging.set_verbosity(logging.WARNING)\n",
        "\n",
        "        #,verbosity=optuna.logging.WARNING\n",
        "\n",
        "        if setupList['max_trials'] >= 1000 or setupList['sampler']=='CmaEsSampler':\n",
        "            study  = optuna.create_study(direction='minimize',sampler=optuna.samplers.CmaEsSampler(warn_independent_sampling=False))\n",
        "        else:\n",
        "            study = optuna.create_study(direction='minimize')\n",
        "\n",
        "        ################\n",
        "        # https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html\n",
        "        ################\n",
        "        # import optuna\n",
        "        # from optuna.samplers import TPESampler\n",
        "\n",
        "        # def objective(trial):\n",
        "        #     x = trial.suggest_uniform('x', -10, 10)\n",
        "        #     return x**2\n",
        "\n",
        "        # study = optuna.create_study(sampler=TPESampler())\n",
        "\n",
        "        # def objective(trial):\n",
        "        #     x = trial.suggest_uniform('x', -100, 100)\n",
        "        #     y = trial.suggest_int('y', -100, 100)\n",
        "        #     return x ** 2 + y ** 2\n",
        "\n",
        "        # search_space = {\n",
        "        #     'x': [-50, 0, 50],\n",
        "        #     'y': [-99, 0, 99]\n",
        "        # }\n",
        "        #study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space))\n",
        "        ############################################################\n",
        "\n",
        "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "        #study.optimize(objective, n_trials=3*3)\n",
        "\n",
        "        study.optimize(objective, n_trials=setupList['max_trials'])\n",
        "\n",
        "        if COLAB:\n",
        "            from IPython.display import display\n",
        "            display(optuna.visualization.plot_optimization_history(study))\n",
        "            display(optuna.visualization.plot_param_importances(study))\n",
        "            #display(optuna.visualization.plot_countour(study))\n",
        "\n",
        "\n",
        "        best_trial = study.best_trial\n",
        "        # Extract best hyperparameters from Optuna study\n",
        "        best_hp = best_trial.params\n",
        "\n",
        "        # This part requires constructing an hp object similar to what is expected by HybridHyperModel\n",
        "        from keras_tuner.engine import hyperparameters as hp_module\n",
        "        best_hp_keras = hp_module.HyperParameters()\n",
        "        for k, v in best_hp.items():\n",
        "            best_hp_keras.Fixed(k, v)\n",
        "\n",
        "        best_model = hypermodel.build(best_hp_keras)  # Build the best model using best_hp_keras\n",
        "\n",
        "    elif setupList['nn_tuner'] == 'keras-tuner':\n",
        "        tuner = RandomSearch(\n",
        "            hypermodel,  # Directly use hypermodel instance\n",
        "            objective='val_loss',\n",
        "            max_trials=setupList['max_trials'],\n",
        "            executions_per_trial=setupList['executions_per_trial'],\n",
        "            directory='hyperparameter_search',\n",
        "            project_name='hybrid_model_tuning'\n",
        "        )\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=setupList['patience'])\n",
        "        tuner.search(X_train, Y_train, epochs=setupList['epochs'], validation_data=validation_dataP, callbacks=[early_stopping])\n",
        "        best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "        best_model = tuner.hypermodel.build(best_hp)  # Build the best model using best_hp from Keras Tuner\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown nn_tuner: {setupList['nn_tuner']}\")\n",
        "\n",
        "    return best_model, best_hp\n",
        "\n",
        "#setupList['trainSetup']='prodPred'\n",
        "#setupList['trainSetup']='prodTrain'\n",
        "\n",
        "    print('\\n')\n",
        "    print(setupList['trainSetup'])\n",
        "    print('\\n')\n",
        "\n",
        "if setupList['trainSetup'] == 'prodTrain':\n",
        "    X_train, Y_train = prepare_input(train_data, time_steps=setupList['time_steps'], exog_names=setupList['exogNames'])\n",
        "    X_test, Y_test = prepare_input(test_data, time_steps=setupList['time_steps'], exog_names=setupList['exogNames'])\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    best_model, best_hp = optimize_hyperparameters(X_train, Y_train, input_shape)\n",
        "    history = best_model.fit(X_train, Y_train, epochs=setupList['epochs'], validation_split=setupList['validation_split'])\n",
        "    #history = best_model.fit(X_train, Y_train, epochs=setupList['epochs'])\n",
        "\n",
        "    loss_df = pd.DataFrame({\n",
        "        'epoch': range(1, len(history.history['loss']) + 1),\n",
        "        'train_loss': history.history['loss'],\n",
        "        'val_loss': history.history['val_loss']\n",
        "    })\n",
        "    loss_df.to_csv(path_manager.get_script_path(\"python_training.csv\"), index=False)\n",
        "\n",
        "    predictions = best_model.predict(X_test)\n",
        "\n",
        "    train_mae = history.history['mae'][-1]\n",
        "    out_of_sample_predictions = predictions\n",
        "\n",
        "    # # Calculate MAE\n",
        "    #train_mae = np.mean(np.abs(Y_train - train_predictions.flatten()))\n",
        "    out_of_sample_mae = np.mean(np.abs(Y_test - out_of_sample_predictions.flatten()))\n",
        "\n",
        "    # Save best parameters to a CSV file\n",
        "    best_params = best_hp if isinstance(best_hp, dict) else best_hp.values\n",
        "    best_params['editDate'] = setupList['editDate']\n",
        "    best_params['algoSrc'] = setupList['algoSrc']\n",
        "    best_params['qRank'] = setupList['qRank']\n",
        "    best_params['part'] = setupList['part']\n",
        "    best_params['lot_type'] = setupList['lot_type']\n",
        "    best_params['train_mae'] = train_mae\n",
        "    best_params['out_of_sample_mae'] = out_of_sample_mae\n",
        "    best_params_df = pd.DataFrame([best_params])\n",
        "\n",
        "    csv_path = path_manager.get_script_path('best_parameters.csv')\n",
        "    if os.path.exists(csv_path):\n",
        "        existing_df = pd.read_csv(csv_path)\n",
        "        updated_df = pd.concat([existing_df, best_params_df], ignore_index=True)\n",
        "    else:\n",
        "        updated_df = best_params_df\n",
        "\n",
        "    updated_df.to_csv(csv_path, index=False)\n",
        "    print(f\"Best parameters have been appended to '{csv_path}'\")\n",
        "\n",
        "    #print(f\"\\nTrain MAE: {train_mae}\")\n",
        "    print(f\"Out-of-Sample MAE: {out_of_sample_mae}\")\n",
        "\n",
        "    if COLAB:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(Y_test, label='True')\n",
        "        plt.plot(predictions, label='Predicted')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Values')\n",
        "        plt.title('Out-of-Sample Data: True vs Predicted')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Training loss plot\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    print(setupList['nn_tuner'])\n",
        "\n",
        "elif setupList['trainSetup'] == 'prodPred':\n",
        "    best_parameters = pd.read_csv(path_manager.get_script_path(\"best_parameters.csv\"))\n",
        "    filtered_parameters = best_parameters[(best_parameters['algoSrc'] == setupList['algoSrc']) &\n",
        "                                          (best_parameters['part'] == setupList['part']) &\n",
        "                                          (best_parameters['lot_type'] == setupList['lot_type']) &\n",
        "                                          (best_parameters['qRank'] == setupList['qRank'])]\n",
        "    best_params = filtered_parameters.sort_values(by=\"out_of_sample_mae\").iloc[0]\n",
        "    input_shape = (setupList['time_steps'], len(setupList['exogNames']))\n",
        "\n",
        "    # Access hyperparameter values directly\n",
        "    layer_type = best_params['layer_type']\n",
        "    filters = int(best_params['filters']) if 'filters' in best_params else None  # Handle optional hyperparameters\n",
        "    kernel_size = int(best_params['kernel_size']) if 'kernel_size' in best_params else None\n",
        "    layer_size = int(best_params['layer_size']) if 'layer_size' in best_params else None\n",
        "    num_heads = int(best_params['num_heads']) if 'num_heads' in best_params else None\n",
        "    ff_dim = int(best_params['ff_dim']) if 'ff_dim' in best_params else None\n",
        "    dropout_rate = best_params['dropout_rate'] if 'dropout_rate' in best_params else None\n",
        "\n",
        "    # Build the model manually using loaded hyperparameters\n",
        "    inputs = Input(shape=input_shape)\n",
        "    if layer_type == 'Conv1D':\n",
        "        x = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(inputs)\n",
        "    elif layer_type == 'LSTM':\n",
        "        x = LSTM(layer_size, return_sequences=True)(inputs)\n",
        "    elif layer_type == 'GRU':\n",
        "        x = GRU(layer_size, return_sequences=True)(inputs)\n",
        "    elif layer_type == 'SimpleRNN':\n",
        "        x = SimpleRNN(layer_size, return_sequences=True)(inputs)\n",
        "    elif layer_type == 'Transformer':\n",
        "        embed_dim = input_shape[-1]\n",
        "        x = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(inputs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown layer_type: {layer_type}\")\n",
        "\n",
        "    x = Attention()(x)\n",
        "    x = Dense(layer_size, activation='relu')(x)\n",
        "    if dropout_rate is not None:\n",
        "        x = Dropout(rate=dropout_rate)(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    best_model = Model(inputs=inputs, outputs=outputs)\n",
        "    best_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    X_all, _ = prepare_input(all_data, time_steps=setupList['time_steps'], exog_names=setupList['exogNames'])\n",
        "    predictions = best_model.predict(X_all[-1].reshape(1, setupList['time_steps'], -1))\n",
        "    print(\"Next predicted value:\", predictions[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRqpEluahrN_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}